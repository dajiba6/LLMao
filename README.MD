# LLMao
欢迎来到 `LLMao` 仓库！这个项目是一个用 Python 实现的大型语言模型（LLM）和代理功能。该仓库的主要目标是记录个人学习过程以及作为新手入门的参考资料.

### 学习路径

1. **学习 Python 基础语法**：专注于理解基本语法、变量、条件控制和类。暂时避免复杂主题；在需要时再学习。 （持续时间：1周+）

2. **使用 Ollama**：下载并在本地运行模型。 （持续时间：2天+）

3. **注册阿里百炼,deepseek或智谱开发者 API**：获取 token，并使用 Python 的 `requests` 库调用 API。练习使用 OpenAI 框架调用 API，同时尝试编写各种任务的提示，如小红书提示词生成或文章改写。 （持续时间：2天+）

   - 原因：智谱提供免费的 大模型 和多模态 API，操作简单且成本低廉。同时兼容 OpenAI 框架，便于后续适应其他模型的调用。

4. **使用Llama-Index**：创建一个检索增强生成（RAG）系统。之后再探索使用纯python或agent狂简编写代理。 （持续时间：1周+）

   - 原因：RAG 和代理能够快速看到效果，减少挫败感。它们也是当前市场的主要业务落地方向，具备此能力即可创建基本的 AI 应用。

5. **玩 Hugging Face**：下载模型和数据集，并根据页面提示进行调用。 （持续时间：1周+）

   - 在这一阶段，考虑显卡需求。熟悉环境后，租用云服务显卡会更加方便。

6. **使用 Unsloth 微调模型**：微调 Qwen、Llama 等模型，探索量化和部署。 （持续时间：1周+）

7. **学习 PyTorch**：深入了解深度学习原理，如 Transformer 架构。 （持续时间：1个月+）

   - 推荐：B站 刘二大人 PyTorch 入门教程；也可以考虑小土堆的教程，但选一个就够了。李沐系列教程可以在后续需要时补充观看。

8. **学习数学和概率等知识**：学习数学和概率的基础知识，然后学习 NumPy、Pandas 等库。 （持续时间：1个月+）

   - 推荐：斯坦福系列课程和唐宇迪的视频教程。

9. **具体项目**：找相关工作，投入时间到具体项目、框架里.

上述每一步未必要完成的很好，到70分即可开始找工作；在面试中了解公司、市场的期望，找到自己的短期具体方向，深入学习；

高效率的学习需要明确自己实现的project。搭建工程，落地实现，比理解理论要花十倍以上的时间。没有具体项目积累，总是虚的，且很快忘记。 

# LLMao

Welcome to the `LLMao` repository! This project is a Python-based implementation of Large Language Models (LLM) and agent functionalities. The primary goal of this repository is to document my learning process and serve as a reference for beginners.

## Learning Path

1. **Learn Basic Python Syntax**: Focus on understanding basic syntax, variables, control flow, and classes. Avoid complex topics for now; learn them as needed. (Duration: 1+ week)

2. **Use Ollama**: Download and run models locally. (Duration: 2+ days)

3. **Register for AliBailian, DeepSeek, or Zhipu Developer API**: Obtain tokens and use Python's `requests` library to call APIs. Practice using the OpenAI framework for API calls and experiment with prompts for tasks such as generating Xiaohongshu prompts or rewriting articles. (Duration: 2+ days)

   - **Reason**: Zhipu provides free large models and multimodal APIs, making it simple and cost-effective. It is also compatible with the OpenAI framework, facilitating future adaptation to other models.

4. **Use Llama-Index**: Create a Retrieval-Augmented Generation (RAG) system. Later, explore implementing agents using pure Python or agent frameworks. (Duration: 1+ week)

   - **Reason**: RAG and agents provide quick results, reducing frustration. They are also key business applications in the current market. Mastering these skills allows for building basic AI applications.

5. **Explore Hugging Face**: Download models and datasets and follow the documentation for usage. (Duration: 1+ week)

   - **At this stage, consider GPU requirements. Once familiar with the environment, renting a cloud-based GPU may be more convenient.**

6. **Fine-tune Models with Unsloth**: Fine-tune models such as Qwen and Llama, exploring quantization and deployment. (Duration: 1+ week)

7. **Learn PyTorch**: Gain an in-depth understanding of deep learning principles, including the Transformer architecture. (Duration: 1+ month)

   - **Recommended Resources**: Bilibili "Liu Erdaren" PyTorch tutorial; alternatively, Xiaotudui’s tutorial, but one is enough. Li Mu’s series can be supplemented as needed.

8. **Learn Mathematics and Probability**: Build foundational knowledge in math and probability before learning NumPy, Pandas, and related libraries. (Duration: 1+ month)

   - **Recommended Resources**: Stanford courses and Tang Yudi’s video tutorials.

9. **Work on Practical Projects**: Gain real-world experience by working on related projects and frameworks.

Not every step needs to be perfected—achieving around 70% proficiency is enough to start applying for jobs. Use interviews to understand industry and market expectations, then refine your short-term focus for in-depth study.

Efficient learning requires clear project goals. **Building and implementing projects is at least ten times more time-consuming than understanding theoretical concepts. Without hands-on project experience, knowledge remains vague and is easily forgotten.**

