{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":167.51353,"end_time":"2025-03-06T04:15:17.0855","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-06T04:12:29.57197","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning Course 2025 HW2\nThe code scripts are from [aideml](https://github.com/WecoAI/aideml) project on github with some modifications.\n\n","metadata":{"papermill":{"duration":0.005727,"end_time":"2025-03-06T04:12:33.518509","exception":false,"start_time":"2025-03-06T04:12:33.512782","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Prerequisites","metadata":{"papermill":{"duration":0.004574,"end_time":"2025-03-06T04:12:33.528056","exception":false,"start_time":"2025-03-06T04:12:33.523482","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T06:13:27.257994Z","iopub.execute_input":"2025-04-02T06:13:27.258306Z","iopub.status.idle":"2025-04-02T06:13:27.480687Z","shell.execute_reply.started":"2025-04-02T06:13:27.258284Z","shell.execute_reply":"2025-04-02T06:13:27.479931Z"}},"outputs":[{"name":"stdout","text":"Wed Apr  2 06:13:27 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   73C    P0             33W /   70W |    4457MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   69C    P0             29W /   70W |    4505MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install dataclasses_json==0.6.4 shutup==0.2.0\n!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n\n!pip install instructor","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-02T08:19:11.793535Z","iopub.execute_input":"2025-04-02T08:19:11.793865Z","iopub.status.idle":"2025-04-02T08:19:21.819338Z","shell.execute_reply.started":"2025-04-02T08:19:11.793840Z","shell.execute_reply":"2025-04-02T08:19:21.818251Z"},"papermill":{"duration":26.387498,"end_time":"2025-03-06T04:12:59.920393","exception":false,"start_time":"2025-03-06T04:12:33.532895","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: dataclasses_json==0.6.4 in /usr/local/lib/python3.10/dist-packages (0.6.4)\nRequirement already satisfied: shutup==0.2.0 in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json==0.6.4) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses_json==0.6.4) (0.9.0)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json==0.6.4) (24.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses_json==0.6.4) (1.0.0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses_json==0.6.4) (4.12.2)\nLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\nRequirement already satisfied: llama-cpp-python==0.3.4 in /usr/local/lib/python3.10/dist-packages (0.3.4)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\nRequirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (5.6.3)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nCollecting instructor\n  Downloading instructor-1.7.8-py3-none-any.whl.metadata (22 kB)\nRequirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor) (3.11.12)\nRequirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.16)\nRequirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor) (3.1.4)\nRequirement already satisfied: jiter<0.9,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.8.2)\nRequirement already satisfied: openai<2.0.0,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (1.57.4)\nRequirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.29.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.11.0a2)\nRequirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.10/dist-packages (from instructor) (2.32.3)\nRequirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (13.9.4)\nRequirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (9.0.0)\nRequirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor) (0.15.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor) (1.18.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor) (3.0.2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (0.28.1)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->instructor) (4.12.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.8.0->instructor) (0.7.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.32.3->instructor) (2025.1.31)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor) (2.19.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor) (1.5.4)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->instructor) (1.2.2)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->instructor) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.52.0->instructor) (0.14.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor) (0.1.2)\nDownloading instructor-1.7.8-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: instructor\nSuccessfully installed instructor-1.7.8\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Download dataset\n\n!gdown --id 1PAI4_3kRWwIPQMscMdGt9HLqZZy1vWSD\n\n# Choose a workable link\n# !gdown --id 1XtF9-hGw2tKe4WvUMW5YE6lj6p1QcWIc\n# !gdown --id 1diswE_9XoT-uII23ucRppau1ErEQkY2y\n# !gdown --id 1BAVMzLZqEgtG8rwog7ttC7xKPw5QTngn\n# !gdown --id 1Ah5uV6cu3Bnz6WfkUuxEZCLqj5k1lbpd\n\n!unzip ML2025Spring-hw2-public.zip","metadata":{"execution":{"iopub.status.busy":"2025-04-02T05:35:19.482241Z","iopub.execute_input":"2025-04-02T05:35:19.482693Z","iopub.status.idle":"2025-04-02T05:35:24.282160Z","shell.execute_reply.started":"2025-04-02T05:35:19.482648Z","shell.execute_reply":"2025-04-02T05:35:24.281136Z"},"papermill":{"duration":4.600341,"end_time":"2025-03-06T04:13:04.530054","exception":false,"start_time":"2025-03-06T04:12:59.929713","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1PAI4_3kRWwIPQMscMdGt9HLqZZy1vWSD\nTo: /kaggle/working/ML2025Spring-hw2-public.zip\n100%|█████████████████████████████████████████| 621k/621k [00:00<00:00, 110MB/s]\nArchive:  ML2025Spring-hw2-public.zip\n   creating: ML2025Spring-hw2-public/\n  inflating: ML2025Spring-hw2-public/sample_submission.csv  \n  inflating: ML2025Spring-hw2-public/test.csv  \n  inflating: ML2025Spring-hw2-public/train.csv  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T05:56:00.852231Z","iopub.execute_input":"2025-04-02T05:56:00.852558Z","iopub.status.idle":"2025-04-02T05:56:00.980334Z","shell.execute_reply.started":"2025-04-02T05:56:00.852535Z","shell.execute_reply":"2025-04-02T05:56:00.979333Z"}},"outputs":[{"name":"stdout","text":"ML2025Spring-hw2-public      qwen2.5-coder-7b-instruct-q8_0.gguf\nML2025Spring-hw2-public.zip\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n# qwen针对编程训练的q8量化14b模型\n#? llama加载模型失败\n!wget https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q8_0.gguf","metadata":{"execution":{"iopub.status.busy":"2025-04-02T05:37:23.864873Z","iopub.execute_input":"2025-04-02T05:37:23.865238Z","iopub.status.idle":"2025-04-02T05:37:57.982303Z","shell.execute_reply.started":"2025-04-02T05:37:23.865208Z","shell.execute_reply":"2025-04-02T05:37:57.981520Z"},"papermill":{"duration":40.274366,"end_time":"2025-03-06T04:13:44.814044","exception":false,"start_time":"2025-03-06T04:13:04.539678","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"--2025-04-02 05:37:23--  https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct-GGUF/resolve/main/qwen2.5-coder-7b-instruct-q8_0.gguf\nResolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.59, 3.165.160.61, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.hf.co/repos/e0/34/e034d2c9ed3c85c64cc3c3a726a18f3975b4303a44cedb3e57551667981abd64/b36a4e1c3ddf2ba6fd5501b926128e5fc6430881caaff07e9629bd18f06f685f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27qwen2.5-coder-7b-instruct-q8_0.gguf%3B+filename%3D%22qwen2.5-coder-7b-instruct-q8_0.gguf%22%3B&Expires=1743575844&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzU3NTg0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UwLzM0L2UwMzRkMmM5ZWQzYzg1YzY0Y2MzYzNhNzI2YTE4ZjM5NzViNDMwM2E0NGNlZGIzZTU3NTUxNjY3OTgxYWJkNjQvYjM2YTRlMWMzZGRmMmJhNmZkNTUwMWI5MjYxMjhlNWZjNjQzMDg4MWNhYWZmMDdlOTYyOWJkMThmMDZmNjg1Zj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=NJs6-XX9uclef4FAgn9mRithrA8TUwc20bQuGTjJz-PU5Rfwt5UIHQbyouILm5qvSCuPxasjbC43EDR1HK3xdlbb%7Eh4wnHtvS%7E8ccNt%7EViyZM5dJ0WETdNLiF2009Igrm7GRwQBVdByNcp7FmidXyC10r7d8Cch7cWIWL0lEaUq1IsqEhr1S12XLK%7EbIQ3acsS2OgFpAYLvuMheZpeN4t9R%7E8xvFs8GdrB%7ElNlgJf8hlGUJSKkfrlbQu8OjZND1rWHYBHAhuzknMFxAaWPwVPpDYcJjk7yXejOEQY-oiLYA6TuVbi9IQcx-KX1DYOT4i2vVGSOmawCfVBPRFg-GO5w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2025-04-02 05:37:24--  https://cdn-lfs-us-1.hf.co/repos/e0/34/e034d2c9ed3c85c64cc3c3a726a18f3975b4303a44cedb3e57551667981abd64/b36a4e1c3ddf2ba6fd5501b926128e5fc6430881caaff07e9629bd18f06f685f?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27qwen2.5-coder-7b-instruct-q8_0.gguf%3B+filename%3D%22qwen2.5-coder-7b-instruct-q8_0.gguf%22%3B&Expires=1743575844&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzU3NTg0NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2UwLzM0L2UwMzRkMmM5ZWQzYzg1YzY0Y2MzYzNhNzI2YTE4ZjM5NzViNDMwM2E0NGNlZGIzZTU3NTUxNjY3OTgxYWJkNjQvYjM2YTRlMWMzZGRmMmJhNmZkNTUwMWI5MjYxMjhlNWZjNjQzMDg4MWNhYWZmMDdlOTYyOWJkMThmMDZmNjg1Zj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=NJs6-XX9uclef4FAgn9mRithrA8TUwc20bQuGTjJz-PU5Rfwt5UIHQbyouILm5qvSCuPxasjbC43EDR1HK3xdlbb%7Eh4wnHtvS%7E8ccNt%7EViyZM5dJ0WETdNLiF2009Igrm7GRwQBVdByNcp7FmidXyC10r7d8Cch7cWIWL0lEaUq1IsqEhr1S12XLK%7EbIQ3acsS2OgFpAYLvuMheZpeN4t9R%7E8xvFs8GdrB%7ElNlgJf8hlGUJSKkfrlbQu8OjZND1rWHYBHAhuzknMFxAaWPwVPpDYcJjk7yXejOEQY-oiLYA6TuVbi9IQcx-KX1DYOT4i2vVGSOmawCfVBPRFg-GO5w__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 108.138.94.64, 108.138.94.16, 108.138.94.37, ...\nConnecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|108.138.94.64|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8098525184 (7.5G) [binary/octet-stream]\nSaving to: ‘qwen2.5-coder-7b-instruct-q8_0.gguf’\n\nqwen2.5-coder-7b-in 100%[===================>]   7.54G   212MB/s    in 34s     \n\n2025-04-02 05:37:57 (229 MB/s) - ‘qwen2.5-coder-7b-instruct-q8_0.gguf’ saved [8098525184/8098525184]\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"message = [\n    {\"role\":\"system\",\"content\":\"你是一个傻逼\"},\n    {\"role\":\"user\",\"content\":\"告诉我如何学习\"},\n]\ngenerate_response(myModel,message)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T03:58:29.308546Z","iopub.execute_input":"2025-04-02T03:58:29.308925Z","iopub.status.idle":"2025-04-02T03:58:37.544191Z","shell.execute_reply.started":"2025-04-02T03:58:29.308900Z","shell.execute_reply":"2025-04-02T03:58:37.543349Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'学习是一个持续的过程，需要不断地努力和实践。以下是一些建议，希望能帮助你更好地学习：\\n\\n1. 制定学习计划：制定一个合理的学习计划，包括每天的学习时间、学习内容和目标。坚持执行计划，逐步提高学习效率。\\n\\n2. 选择合适的学习方法：根据自己的学习习惯和特点，选择适合自己的学习方法。例如，有的人适合通过阅读和笔记来学习，有的人适合通过实践和实验来学习。\\n\\n3. 寻找学习资源：利用各种学习资源，如书籍、网络课程、视频教程等，丰富自己的学习内容和方式。\\n\\n4. 培养学习兴趣：找到自己感兴趣的学习内容，培养学习兴趣。兴趣是最好的老师，只有对学习感兴趣，才能更好地投入其中。\\n\\n5. 反思和总结：在学习过程中，及时反思和总结自己的学习情况，找出不足之处，及时调整学习方法和计划。\\n\\n6. 保持积极心态：学习过程中难免会遇到困难和挫折，保持积极心态，相信自己能够克服困难，不断进步。\\n\\n希望这些建议对你有所帮助，祝你学习顺利！'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nmyModel = Llama(\n    # ========================== TODO: try different LLM ==========================\n    # Before changing LLM, restart the session!\n    \"qwen2.5-coder-7b-instruct-q8_0.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=8192,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory.\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    This function will inference the model with given messages.\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=4096,    # This argument is how many tokens the model can generate.\n        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. We suggest setting the temperature value to 0 for reproducibility.\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"execution":{"iopub.status.busy":"2025-04-02T05:56:06.907486Z","iopub.execute_input":"2025-04-02T05:56:06.907850Z","iopub.status.idle":"2025-04-02T05:56:09.858636Z","shell.execute_reply.started":"2025-04-02T05:56:06.907818Z","shell.execute_reply":"2025-04-02T05:56:09.857993Z"},"papermill":{"duration":5.576543,"end_time":"2025-03-06T04:13:57.275315","exception":false,"start_time":"2025-03-06T04:13:51.698772","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","text":"llama_new_context_with_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Functions","metadata":{"papermill":{"duration":0.017836,"end_time":"2025-03-06T04:13:57.311817","exception":false,"start_time":"2025-03-06T04:13:57.293981","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Utils","metadata":{"papermill":{"duration":0.017541,"end_time":"2025-03-06T04:13:57.347121","exception":false,"start_time":"2025-03-06T04:13:57.32958","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Define a function to save the best solution and other good solutions to files.\ndef save_run(cfg, journal):\n    # Retrieve and save the best found solution.\n    best_node = journal.get_best_node(only_good=False)  # Get the best node.\n    with open(\"/kaggle/working/best_solution.py\", \"w\") as f:\n        f.write(best_node.code)\n\n    good_nodes = journal.get_good_nodes()  # Retrieve all good solution nodes.\n    for i, node in enumerate(good_nodes):\n        filename = f\"/kaggle/working/good_solution_{i}.py\"\n        with open(filename, \"w\") as f:\n            f.write(node.code)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T05:56:14.825293Z","iopub.execute_input":"2025-04-02T05:56:14.825661Z","iopub.status.idle":"2025-04-02T05:56:14.831864Z","shell.execute_reply.started":"2025-04-02T05:56:14.825632Z","shell.execute_reply":"2025-04-02T05:56:14.830824Z"},"papermill":{"duration":0.027881,"end_time":"2025-03-06T04:13:57.394758","exception":false,"start_time":"2025-03-06T04:13:57.366877","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Interpreter (DO NOT MODIFY THIS CELL)","metadata":{"papermill":{"duration":0.016895,"end_time":"2025-03-06T04:13:57.429444","exception":false,"start_time":"2025-03-06T04:13:57.412549","status":"completed"},"tags":[]}},{"cell_type":"code","source":"\"\"\"\nDO NOT MODIFY THIS CELL\n\nPython interpreter for executing code snippets and capturing their output.\n\"\"\"\n\n\nimport logging\nimport os\nimport queue\nimport signal\nimport sys\nimport time\nimport traceback\nimport zipfile\nfrom pathlib import Path\nfrom shutil import rmtree\nimport shutil\nfrom multiprocessing import Process, Queue\nfrom typing import Hashable, cast\n\nimport humanize\nimport rich\nimport shutup\nfrom rich.logging import RichHandler\nfrom rich.syntax import Syntax\nfrom dataclasses import dataclass\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass\nclass ExecutionResult(DataClassJsonMixin):\n    \"\"\"\n    Result of executing a code snippet in the interpreter.\n    Contains the output, execution time, and exception information.\n    \"\"\"\n    term_out: list[str]\n    exec_time: float\n    exc_type: str | None\n    exc_info: dict | None = None\n    exc_stack: list[tuple] | None = None\n\ndef exception_summary(e, exec_file_name):\n    \"\"\"Generates a string that summarizes an exception and its stack trace\"\"\"\n    tb_lines = traceback.format_exception(e)\n    # Combine the traceback lines into a single string, skipping lines that contain \"importlib\".\n    tb_str = \"\".join(\n        [\n            line\n            for line in tb_lines\n            # if \"importlib\" not in line  # Filter out unwanted traceback lines.\n        ]\n    )\n\n    exc_info = {}\n    if hasattr(e, \"args\"):\n        exc_info[\"args\"] = [str(i) for i in e.args]  # Store the exception arguments as strings.\n    for att in [\"name\", \"msg\", \"obj\"]:\n        if hasattr(e, att):\n            exc_info[att] = str(getattr(e, att))  # Store additional attributes if available.\n\n    tb = traceback.extract_tb(e.__traceback__)  # Extract the traceback information.\n    # Create a list of tuples for each frame in the traceback.\n    exc_stack = [(t.filename, t.lineno, t.name, t.line) for t in tb]\n\n    return tb_str, e.__class__.__name__, exc_info, exc_stack  # Return the formatted traceback and exception details.\n\n# Define a class that redirects write operations to a multiprocessing queue.\nclass RedirectQueue:\n    def __init__(self, queue, timeout=5):\n        self.queue = queue  # Store the provided queue.\n        self.timeout = timeout  # Set the timeout for queue operations.\n\n    def write(self, msg):\n        try:\n            self.queue.put(msg, timeout=self.timeout)  # Attempt to put the message into the queue.\n        except queue.Full:\n            print.warning(\"Queue write timed out\")  # Warn if the queue is full and the write times out.\n\n    def flush(self):\n        pass  # No operation is needed for flushing in this context.\n\n# Define the Interpreter class that simulates a standalone Python REPL.\nclass Interpreter:\n    def __init__(\n        self,\n        timeout: int = 3600,  # Default timeout of 3600 seconds.\n        agent_file_name: str = \"runfile.py\",  # Default file name for writing the agent's code.\n    ):\n        \"\"\"\n        Simulates a standalone Python REPL with an execution time limit.\n\n        Args:\n            timeout (int, optional): Timeout for each code execution step. Defaults to 3600.\n            agent_file_name (str, optional): The name for the agent's code file. Defaults to \"runfile.py\".\n        \"\"\"\n        self.timeout = timeout  # Save the timeout value.\n        self.agent_file_name = agent_file_name  # Save the agent file name.\n        self.process: Process = None  # Initialize the process attribute (will hold the child process).\n\n    def child_proc_setup(self, result_outq: Queue) -> None:\n        # Import shutup to suppress warnings in the child process.\n        import shutup\n\n        shutup.mute_warnings()  # Mute all warnings before further execution.\n\n        # Redirect both stdout and stderr to the provided result queue.\n        # trunk-ignore(mypy/assignment)\n        sys.stdout = sys.stderr = RedirectQueue(result_outq)\n\n    def _run_session(\n        self, code_inq: Queue, result_outq: Queue, event_outq: Queue\n    ) -> None:\n        self.child_proc_setup(result_outq)  # Set up the child process for capturing output.\n\n        global_scope: dict = {}  # Create an empty dictionary to serve as the global scope.\n        while True:  # Continuously wait for new code to execute.\n            code = code_inq.get()  # Retrieve code from the code input queue.\n            with open(self.agent_file_name, \"w\") as f:  # Open the agent file for writing.\n                f.write(code)  # Write the received code into the file.\n\n            event_outq.put((\"state:ready\",))  # Signal that the interpreter is ready to execute the code.\n            try:\n                # Compile and execute the code within the global scope.\n                exec(compile(code, self.agent_file_name, \"exec\"), global_scope)\n            except BaseException as e:\n                # If an exception occurs, generate a summary of the exception.\n                tb_str, e_cls_name, exc_info, exc_stack = exception_summary(\n                    e,\n                    self.agent_file_name,\n                )\n                result_outq.put(tb_str)  # Put the traceback string into the result queue.\n                if e_cls_name == \"KeyboardInterrupt\":\n                    e_cls_name = \"TimeoutError\"  # Convert a KeyboardInterrupt into a TimeoutError.\n\n                event_outq.put((\"state:finished\", e_cls_name, exc_info, exc_stack))  # Signal that execution finished with an error.\n            else:\n                event_outq.put((\"state:finished\", None, None, None))  # Signal that execution finished successfully.\n\n            os.remove(self.agent_file_name)  # Remove the agent file after execution.\n\n            result_outq.put(\"<|EOF|>\")  # Put an EOF marker to indicate the end of output.\n\n    def create_process(self) -> None:\n        # Create three queues for communication with the child process:\n        # - code_inq: for sending code to execute.\n        # - result_outq: for receiving output from the execution.\n        # - event_outq: for receiving state events (like ready and finished).\n        # trunk-ignore(mypy/var-annotated)\n        self.code_inq, self.result_outq, self.event_outq = Queue(), Queue(), Queue()\n        self.process = Process(\n            target=self._run_session,  # Set the target function for the child process.\n            args=(self.code_inq, self.result_outq, self.event_outq),  # Provide the necessary queues as arguments.\n        )\n        self.process.start()  # Start the child process.\n\n    def cleanup_session(self):\n        if self.process is None:  # If there is no process, nothing to clean up.\n            return\n        try:\n            # Attempt to terminate the child process gracefully.\n            self.process.terminate()  # Request the process to terminate.\n            self.process.join(timeout=0.5)  # Wait for the process to finish with a 0.5-second timeout.\n\n            if self.process.exitcode is None:  # If the process is still running,\n                self.process.kill()  # Forcefully kill the process.\n                self.process.join(timeout=0.5)  # Wait again for termination.\n\n                if self.process.exitcode is None:  # If the process still hasn't terminated,\n                    os.kill(self.process.pid, signal.SIGKILL)  # Send a SIGKILL signal.\n        except Exception as e:\n            print(f\"Error during process cleanup: {e}\")  # Print an error message if cleanup fails.\n        finally:\n            if self.process is not None:  # If the process exists,\n                self.process.close()  # Close the process.\n                self.process = None  # Reset the process attribute to None.\n\n    def run(self, code: str, reset_session=True) -> ExecutionResult:\n        \"\"\"\n        Execute the provided Python command in a separate process and return its output.\n\n        Parameters:\n            code (str): Python code to execute.\n            reset_session (bool, optional): Whether to reset the interpreter session before executing the code. Defaults to True.\n\n        Returns:\n            ExecutionResult: Object containing the output and metadata of the code execution.\n        \"\"\"\n\n        if reset_session:\n            if self.process is not None:\n                # If a previous process exists, clean it up before starting a new one.\n                self.cleanup_session()\n            self.create_process()  # Create a new child process.\n        else:\n            # For the first execution, reset_session must be True.\n            assert self.process is not None\n\n        assert self.process.is_alive()  # Ensure that the child process is running.\n\n        self.code_inq.put(code)  # Send the code to the child process via the queue.\n\n        # Wait for the child process to signal that it is ready.\n        try:\n            state = self.event_outq.get(timeout=10)  # Wait up to 10 seconds for the \"state:ready\" event.\n        except queue.Empty:\n            msg = \"REPL child process failed to start execution\"\n            print.critical(msg)  # Log a critical error if the process does not start.\n            while not self.result_outq.empty():\n                continue  # Drain the result queue.\n            raise RuntimeError(msg) from None\n        assert state[0] == \"state:ready\", state  # Verify that the received state is \"state:ready\".\n        start_time = time.time()  # Record the start time of execution.\n\n        child_in_overtime = False  # Flag to indicate if the child process has exceeded the timeout.\n\n        while True:\n            try:\n                # Try to get the finished state from the child process.\n                state = self.event_outq.get(timeout=1)  # Wait for the \"state:finished\" event.\n                assert state[0] == \"state:finished\", state  # Ensure the state is \"state:finished\".\n                exec_time = time.time() - start_time  # Calculate the total execution time.\n                break  # Exit the loop if execution is finished.\n            except queue.Empty:\n                # If no event is received, check whether the process is still alive.\n                if not child_in_overtime and not self.process.is_alive():\n                    msg = \"REPL child process died unexpectedly\"\n                    raise RuntimeError(msg) from None\n\n                # If the process is still running, check if it has exceeded the timeout.\n                if self.timeout is None:\n                    continue\n                running_time = time.time() - start_time  # Determine the running time.\n                if running_time > self.timeout:\n                    print(f\"Execution exceeded timeout of {self.timeout}s\")  # Log a timeout message.\n                    os.kill(self.process.pid, signal.SIGINT)  # Send SIGINT to the process.\n                    child_in_overtime = True  # Mark that the process is now in overtime.\n\n                    # If the process exceeds the timeout by more than 5 seconds, force cleanup.\n                    if running_time > self.timeout + 5:\n                        self.cleanup_session()  # Clean up the child process.\n\n                        state = (None, \"TimeoutError\", {}, [])  # Set state to indicate a timeout error.\n                        exec_time = self.timeout  # Set the execution time to the timeout limit.\n                        break\n\n        output: list[str] = []  # Initialize a list to collect output lines.\n        # Collect all output from the result queue until the EOF marker is encountered.\n        start_collect = time.time()  # Record the start time for output collection.\n        while not self.result_outq.empty() or not output or output[-1] != \"<|EOF|>\":\n            try:\n                # If output collection exceeds 5 seconds, log a warning.\n                if time.time() - start_collect > 5:\n                    print.warning(\"Output collection timed out\")\n                    break\n                output.append(self.result_outq.get(timeout=1))  # Append the next line of output.\n            except queue.Empty:\n                continue  # Continue if no output is available immediately.\n        output.pop()  # Remove the EOF marker from the output list.\n\n        # Extract exception information from the finished state.\n        e_cls_name, exc_info, exc_stack = state[1:]\n\n        if e_cls_name == \"TimeoutError\":\n            # Append a timeout error message to the output if a timeout occurred.\n            output.append(\n                f\"TimeoutError: Execution exceeded the time limit of {humanize.naturaldelta(self.timeout)}\"\n            )\n        else:\n            # Append the execution time information to the output.\n            output.append(\n                f\"Execution time: {humanize.naturaldelta(exec_time)} seconds (time limit is {humanize.naturaldelta(self.timeout)}).\"\n            )\n        # Return an ExecutionResult object with all the execution details.\n        return ExecutionResult(output, exec_time, e_cls_name, exc_info, exc_stack)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T05:56:22.057490Z","iopub.execute_input":"2025-04-02T05:56:22.057799Z","iopub.status.idle":"2025-04-02T05:56:22.273352Z","shell.execute_reply.started":"2025-04-02T05:56:22.057772Z","shell.execute_reply":"2025-04-02T05:56:22.272470Z"},"papermill":{"duration":0.289598,"end_time":"2025-03-06T04:13:57.735847","exception":false,"start_time":"2025-03-06T04:13:57.446249","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Nodes\n","metadata":{"papermill":{"duration":0.017568,"end_time":"2025-03-06T04:13:57.770796","exception":false,"start_time":"2025-03-06T04:13:57.753228","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import time\nimport uuid\nfrom dataclasses import dataclass, field\nfrom typing import Literal, Optional\n\nfrom dataclasses_json import DataClassJsonMixin\n\n\n@dataclass(eq=False)\nclass Node(DataClassJsonMixin):\n    \"\"\"A single node in the solution tree. Contains code, execution results, and evaluation information.\"\"\"\n\n    # ---- code & plan ----\n    code: str\n    plan: str = field(default=None, kw_only=True)  # type: ignore\n\n    # ---- general attrs ----\n    step: int = field(default=None, kw_only=True)  # type: ignore\n    id: str = field(default_factory=lambda: uuid.uuid4().hex, kw_only=True)\n    ctime: float = field(default_factory=lambda: time.time(), kw_only=True)\n    parent: Optional[\"Node\"] = field(default=None, kw_only=True)\n    children: set[\"Node\"] = field(default_factory=set, kw_only=True)\n\n    # ---- execution info ----\n    _term_out: list[str] = field(default=None, kw_only=True)  # type: ignore\n    exec_time: float = field(default=None, kw_only=True)  # type: ignore\n    exc_type: str | None = field(default=None, kw_only=True)\n    exc_info: dict | None = field(default=None, kw_only=True)\n    exc_stack: list[tuple] | None = field(default=None, kw_only=True)\n\n    # ---- evaluation ----\n    # post-execution result analysis (findings/feedback)\n    analysis: str = field(default=None, kw_only=True)  # type: ignore\n    metric: float = field(default=None, kw_only=True)  # type: ignore\n    # whether the agent decided that the code is buggy\n    # -> always True if exc_type is not None or no valid metric\n    is_buggy: bool = field(default=None, kw_only=True)  # type: ignore\n\n    def __post_init__(self) -> None:\n        if self.parent is not None:\n            self.parent.children.add(self)\n\n    @property\n    def stage_name(self) -> Literal[\"draft\", \"debug\", \"improve\"]:\n        \"\"\"\n        Return the stage of the node:\n        - \"stage\" if the node is an initial solution draft\n        - \"debug\" if the node is the result of a debugging step\n        - \"improve\" if the node is the result of an improvement step\n        \"\"\"\n        if self.parent is None:\n            return \"draft\"\n        return \"debug\" if self.parent.is_buggy else \"improve\"\n\n    def absorb_exec_result(self, exec_result: ExecutionResult):\n        \"\"\"Absorb the result of executing the code from this node.\"\"\"\n        self._term_out = exec_result.term_out\n        self.exec_time = exec_result.exec_time\n        self.exc_type = exec_result.exc_type\n        self.exc_info = exec_result.exc_info\n        self.exc_stack = exec_result.exc_stack\n\n    @property\n    def term_out(self) -> str:\n        \"\"\"Get the terminal output of the code execution (after truncating it).\"\"\"\n        return trim_long_string(\"\".join(self._term_out))\n\n    @property\n    def is_leaf(self) -> bool:\n        \"\"\"Check if the node is a leaf node in the solution tree.\"\"\"\n        return not self.children\n\n    def __eq__(self, other):\n        return isinstance(other, Node) and self.id == other.id\n\n    def __hash__(self):\n        return hash(self.id)\n\n    @property\n    def debug_depth(self) -> int:\n        \"\"\"\n        Length of the current debug path\n        - 0 if the node is not a debug node (parent is not buggy)\n        - 1 if the parent is buggy but the skip parent isn't\n        - n if there were n consecutive debugging steps\n        \"\"\"\n        if self.stage_name != \"debug\":\n            return 0\n        return self.parent.debug_depth + 1  # type: ignore\n\n@dataclass\nclass Journal(DataClassJsonMixin):\n    \"\"\"A collection of nodes representing the solution tree.\"\"\"\n\n    nodes: list[Node] = field(default_factory=list)\n\n    def __getitem__(self, idx: int) -> Node:\n        return self.nodes[idx]\n\n    def __len__(self) -> int:\n        \"\"\"Return the number of nodes in the journal.\"\"\"\n        return len(self.nodes)\n\n    def append(self, node: Node) -> None:\n        \"\"\"Append a new node to the journal.\"\"\"\n        node.step = len(self.nodes)\n        self.nodes.append(node)\n\n    @property\n    def draft_nodes(self) -> list[Node]:\n        \"\"\"Return a list of nodes representing intial coding drafts\"\"\"\n        return [n for n in self.nodes if n.parent is None]\n\n    @property\n    def buggy_nodes(self) -> list[Node]:\n        \"\"\"Return a list of nodes that are considered buggy by the agent.\"\"\"\n        return [n for n in self.nodes if n.is_buggy]\n\n    @property\n    def good_nodes(self) -> list[Node]:\n        \"\"\"Return a list of nodes that are not considered buggy by the agent.\"\"\"\n        return [n for n in self.nodes if not n.is_buggy]\n\n    def get_metric_history(self) -> list[float]:\n        \"\"\"Return a list of all metric values in the journal.\"\"\"\n        return [n.metric for n in self.nodes]\n\n    def get_good_nodes(self) -> Node:\n        return [n for n in self.nodes if not n.is_buggy]\n\n    def get_best_node(self, only_good=True) -> None | Node:\n        \"\"\"Return the best solution found so far (node with the highest validation metric).\"\"\"\n        if only_good:\n            nodes = self.good_nodes\n            if not nodes:\n                return None\n        else:\n            nodes = self.nodes\n        return min(nodes, key=lambda n: n.metric)\n\n    def generate_summary(self, include_code: bool = False) -> str:\n        \"\"\"Generate a summary of the journal for the agent.\"\"\"\n        summary = []\n        for n in self.good_nodes:\n            summary_part = f\"Design: {n.plan}\\n\"\n            if include_code:\n                summary_part += f\"Code: {n.code}\\n\"\n            summary_part += f\"Results: {n.analysis}\\n\"\n            summary_part += f\"Validation Metric (Mean Squared Error): {n.metric}\\n\"\n            summary.append(summary_part)\n        return \"\\n-------------------------------\\n\".join(summary)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:16:23.065611Z","iopub.execute_input":"2025-04-02T09:16:23.065923Z","iopub.status.idle":"2025-04-02T09:16:23.080957Z","shell.execute_reply.started":"2025-04-02T09:16:23.065900Z","shell.execute_reply":"2025-04-02T09:16:23.080089Z"},"papermill":{"duration":0.036157,"end_time":"2025-03-06T04:13:57.824253","exception":false,"start_time":"2025-03-06T04:13:57.788096","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"### Agent","metadata":{"papermill":{"duration":0.016855,"end_time":"2025-03-06T04:13:57.858963","exception":false,"start_time":"2025-03-06T04:13:57.842108","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# 添加llm结构化输出\nimport instructor\nfrom pydantic import BaseModel\n\nclass EvaluationOutput(BaseModel):\n    is_buggy: bool\n    metric: float\n    summary: str\n\ndef generate_evaluation_structured(_model: Llama, _messages: str) -> str:\n    # add structured output\n    create = instructor.patch(\n        create=_model.create_chat_completion_openai_v1,\n        mode=instructor.Mode.JSON_SCHEMA,\n    )\n\n    response = create(\n        messages = _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=4096,\n        temperature=0,\n        response_model=EvaluationOutput,\n    )\n    return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:16:29.029527Z","iopub.execute_input":"2025-04-02T09:16:29.029893Z","iopub.status.idle":"2025-04-02T09:16:29.035775Z","shell.execute_reply.started":"2025-04-02T09:16:29.029854Z","shell.execute_reply":"2025-04-02T09:16:29.034838Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"## 结构化输出测试\ntest_messages =[\n    {\"role\":\"system\",\"content\":\"你是一个人工智能助手\"},\n    {\"role\":\"user\",\"content\":\"print('hello, world!')\"}\n] \ntest_res = generate_evaluation_structured(myModel,test_messages)\nprint(test_res)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:11:35.561081Z","iopub.execute_input":"2025-04-02T09:11:35.561370Z","iopub.status.idle":"2025-04-02T09:11:39.809683Z","shell.execute_reply.started":"2025-04-02T09:11:35.561346Z","shell.execute_reply":"2025-04-02T09:11:39.808794Z"}},"outputs":[{"name":"stdout","text":"is_buggy=False metric=0.0 summary=\"The provided code snippet is a simple print statement in Python, which does not contain any bugs or errors. It simply outputs the string 'hello, world!' to the console.\"\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"import random\nfrom typing import Any, Callable, cast\n\nimport re\nimport sys\nimport json\nimport humanize\nimport instructor\nfrom pydantic import BaseModel\n\nExecCallbackType = Callable[[str, bool], ExecutionResult]\n\n\nclass Agent:\n    def __init__(\n        self,\n        cfg,\n        journal: Journal,\n    ):\n        super().__init__()\n        self.cfg = cfg\n        self.journal = journal\n        self.data_preview: str | None = None\n\n    def search_policy(self) -> Node | None:\n        \"\"\"Select a node to work on (or None to draft a new node).\"\"\"\n        search_cfg = self.cfg.agent.search\n\n        # initial drafting\n        if len(self.journal.draft_nodes) < search_cfg.num_drafts:\n            return None\n\n        # debugging\n        if random.random() < search_cfg.debug_prob:\n            # nodes that are buggy + leaf nodes + debug depth < max debug depth\n            debuggable_nodes = [\n                n\n                for n in self.journal.buggy_nodes\n                if n.is_leaf\n            ]\n            if debuggable_nodes:\n                return random.choice(debuggable_nodes)\n\n\n        # back to drafting if no nodes to improve\n        good_nodes = self.journal.good_nodes\n        if not good_nodes:\n            return None\n\n        # greedy\n        greedy_node = self.journal.get_best_node()\n\n        return greedy_node\n\n\n    def plan_and_code_query(self, system_message, user_message, retries=3) -> tuple[str, str]:\n        \"\"\"Generate a natural language plan + code in the same LLM call and split them apart.\"\"\"\n        completion_text = None\n        for _ in range(retries):\n\n            response = generate_response(\n                myModel,\n                _messages=[\n                    {'role': 'system', \"content\": system_message},\n                    {'role': 'user', \"content\": user_message}\n                ]\n            )\n            completion_text = response\n            code = extract_code(completion_text)\n            nl_text = extract_text_up_to_code(completion_text)\n\n            if code:\n                return nl_text, code\n\n            print(\"Plan + code extraction failed, retrying...\")\n        print(\"Final plan + code extraction attempt failed, giving up...\")\n        return \"\", completion_text \n\n    def _draft(self) -> Node:\n\n        # ================ TODO: ask LLM agents to come up with a solution and then implement ================\n\n        system_prompt = \"You are an AI agent.\"\n\n        user_prompt = [\n            \"You have to come up with a solution for machine learning task and then implement this solution in Python.\"\n            f\"The task is to {str(self.cfg.task_goal)} \",\n            f'All the provided input data is stored in \"{self.cfg.data_dir}\" directory.',\n            f\"{str(self.data_preview)}\",\n            'You have to save the predictions result on testing set in \"/kaggle/working/submission.csv\".',\n            'Note that the testing file DOES NOT have the target column.'\n        ]\n\n        system_message = system_prompt\n        user_message = \"\\n\".join(user_prompt)\n        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n        return Node(plan=plan, code=code)\n\n    def _improve(self, parent_node: Node) -> Node:\n\n        # ================  TODO: ask LLM agent to improve drafts ================\n\n        system_prompt = \"You are an AI assistant.\"\n\n        user_prompt = [\n            f\"Task description: {str(self.cfg.task_goal)} \"\n            f\"Memory: {str(self.journal.generate_summary())} \"\n            f\"Previous solution: Code: {str(wrap_code(parent_node.code))} \",\n            \"help me to improve previous solution.\"\n        ]\n        system_message = system_prompt\n        user_message = \" \".join(user_prompt)\n        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n        return Node(plan=plan, code=code, parent=parent_node)\n\n    def _debug(self, parent_node: Node) -> Node:\n\n        # ================  TODO: ask LLM agent to debug ================\n        system_prompt = \"You are an AI agent.\"\n\n\n        user_prompt = [\n            f\"Task description: {str(self.cfg.task_goal)}\\n\\n\",\n            f\"Previous (buggy) implementation: {str(wrap_code(parent_node.code))}\\n\\n\",\n            f\"Execution output: {str(wrap_code(parent_node.term_out, lang=''))}\\n\\n\",\n            str(self.data_preview),\n            \"help me to debug.\"\n        ]\n\n        system_message = system_prompt\n        user_message = \" \".join(user_prompt)\n\n        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n        return Node(plan=plan, code=code, parent=parent_node)\n\n    def update_data_preview(\n        self,\n    ):\n        self.data_preview = data_preview_generate(cfg.data_dir)\n\n    def step(self, exec_callback: ExecCallbackType):\n        if not self.journal.nodes or self.data_preview is None:\n            self.update_data_preview()\n\n        parent_node = self.search_policy()\n\n        if parent_node is None:\n            result_node = self._draft()\n        elif parent_node.is_buggy:\n            result_node = self._debug(parent_node)\n        else:\n            result_node = self._improve(parent_node)\n\n        self.parse_exec_result(\n            node=result_node,\n            exec_result=exec_callback(result_node.code, True),\n        )\n        self.journal.append(result_node)\n\n    def parse_exec_result(self, node: Node, exec_result: ExecutionResult):\n        node.absorb_exec_result(exec_result)\n\n        system_prompt = \"You are an AI assistant.\"\n\n        # ================  TODO: ask LLM agent to extract evaluation result from the execution output. ================\n        # save log file\n        user_prompt = f\"\"\"\n            The task is:\n            {self.cfg.task_goal}\n\n            The code implementation is:\n            {wrap_code(node.code)}\n\n            The execution output is:\n            {wrap_code(node.term_out, lang=\"\")}\n\n            extract evaluation result from the execution output.\n        \"\"\"\n\n        system_message = system_prompt\n        user_message = \" \".join(user_prompt)\n\n        \n        # response = generate_response(\n        #     myModel,\n        #     _messages=[\n        #         {'role': 'system', \"content\": system_message},\n        #         {'role': 'user', \"content\": user_message}\n        #     ]\n        # )\n\n\n        response = generate_evaluation_structured(\n            myModel, \n            _messages=[\n                {'role': 'system', \"content\": system_message},\n                {'role': 'user', \"content\": user_message}\n            ]\n        )\n        \n        # ================  TODO: evaluation ================\n        # you can force the LLM to structure the output to extract the metric\n        # reference: https://python.useinstructor.com/integrations/llama-cpp-python/#llama-cpp-python\n        node.analysis = response.summary\n        node.is_buggy = (\n            response.is_buggy\n            or node.exc_type is not None\n            or response.metric is None\n        )\n\n        # node.is_buggy = False\n        node.metric = response.metric\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:16:34.342329Z","iopub.execute_input":"2025-04-02T09:16:34.342610Z","iopub.status.idle":"2025-04-02T09:16:34.356177Z","shell.execute_reply.started":"2025-04-02T09:16:34.342587Z","shell.execute_reply":"2025-04-02T09:16:34.355229Z"},"papermill":{"duration":0.282813,"end_time":"2025-03-06T04:13:58.158882","exception":false,"start_time":"2025-03-06T04:13:57.876069","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"### Text Processing","metadata":{"papermill":{"duration":0.017566,"end_time":"2025-03-06T04:13:58.193923","exception":false,"start_time":"2025-03-06T04:13:58.176357","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import json\nimport re\n\ndef wrap_code(code: str, lang=\"python\") -> str:\n    \"\"\"Wraps code with three backticks.\"\"\"\n    return f\"```{lang}\\n{code}\\n```\"\n\n\ndef is_valid_python_script(script):\n    \"\"\"Check if a script is a valid Python script.\"\"\"\n    try:\n        compile(script, \"<string>\", \"exec\")\n        return True\n    except SyntaxError:\n        return False\n\n\ndef extract_jsons(text):\n    \"\"\"Extract all JSON objects from the text. Caveat: This function cannot handle nested JSON objects.\"\"\"\n    json_objects = []\n\n    # Find {} by regular expression\n    matches = re.findall(r\"\\{.*?\\}\", text, re.DOTALL)\n\n    # Try to transform string into json objects\n    for match in matches:\n        try:\n            json_obj = json.loads(match)\n            json_objects.append(json_obj)\n        except json.JSONDecodeError:\n            pass\n\n    return json_objects\n\ndef trim_long_string(string, threshold=5100, k=2500):\n    # Check if the length of the string is longer than the threshold\n    if len(string) > threshold:\n        # Output the first k and last k characters\n        first_k_chars = string[:k]\n        last_k_chars = string[-k:]\n\n        truncated_len = len(string) - 2 * k\n\n        return f\"{first_k_chars}\\n ... [{truncated_len} characters truncated] ... \\n{last_k_chars}\"\n    else:\n        return string\n\ndef extract_code(text):\n    \"\"\"Extract python code blocks from the text.\"\"\"\n    parsed_codes = []\n\n    # When code is in a text or python block\n    matches = re.findall(r\"```(python)?\\n*(.*?)\\n*```\", text, re.DOTALL)\n    for match in matches:\n        code_block = match[1]\n        parsed_codes.append(code_block)\n\n    # When the entire text is code or backticks of the code block is missing\n    if len(parsed_codes) == 0:\n        matches = re.findall(r\"^(```(python)?)?\\n?(.*?)\\n?(```)?$\", text, re.DOTALL)\n        if matches:\n            code_block = matches[0][2]\n            parsed_codes.append(code_block)\n\n    # validate the parsed codes\n    valid_code_blocks = [\n        c for c in parsed_codes if is_valid_python_script(c)\n    ]\n    return \"\\n\\n\".join(valid_code_blocks)\n\ndef extract_text_up_to_code(s):\n    \"\"\"Extract (presumed) natural language text up to the start of the first code block.\"\"\"\n    if \"```\" not in s:\n        return \"\"\n    return s[: s.find(\"```\")].strip()\n\n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:16:38.962692Z","iopub.execute_input":"2025-04-02T09:16:38.963033Z","iopub.status.idle":"2025-04-02T09:16:38.970948Z","shell.execute_reply.started":"2025-04-02T09:16:38.963007Z","shell.execute_reply":"2025-04-02T09:16:38.970071Z"},"papermill":{"duration":0.027327,"end_time":"2025-03-06T04:13:58.238391","exception":false,"start_time":"2025-03-06T04:13:58.211064","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"### Feature Selection","metadata":{"papermill":{"duration":0.029221,"end_time":"2025-03-06T04:13:58.288714","exception":false,"start_time":"2025-03-06T04:13:58.259493","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\nimport humanize\nimport pandas as pd\n\n\ndef preview_csv(p: Path) -> str:\n    \"\"\"Generate a textual preview of a csv file\"\"\"\n\n    df = pd.read_csv(p)\n\n    out = []\n\n    out.append(f\"-> {str(p)} has {df.shape[0]} rows and {df.shape[1]} columns.\")\n\n    # ================  TODO: Tell LLM agents which feature is useful for prediction ================\n\n    cols = df.columns.tolist()\n    cols_str = \", \".join(cols)\n    res = f\"The columns are: {cols_str}\"\n\n    out.append(res)\n\n    return \"\\n\".join(out)\n\ndef data_preview_generate(base_path):\n    \"\"\"\n    Generate a textual preview of a directory\n    \"\"\"\n\n    result = []\n    files = [p for p in Path(base_path).iterdir()]\n    for f in sorted(files):\n        result.append(preview_csv(f))\n\n    result = \"\\n\\n\".join(result)\n\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:16:43.910047Z","iopub.execute_input":"2025-04-02T09:16:43.910328Z","iopub.status.idle":"2025-04-02T09:16:44.178613Z","shell.execute_reply.started":"2025-04-02T09:16:43.910306Z","shell.execute_reply":"2025-04-02T09:16:44.178008Z"},"papermill":{"duration":1.450056,"end_time":"2025-03-06T04:13:59.758909","exception":false,"start_time":"2025-03-06T04:13:58.308853","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## Config","metadata":{"papermill":{"duration":0.01766,"end_time":"2025-03-06T04:13:59.794899","exception":false,"start_time":"2025-03-06T04:13:59.777239","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\n# DO NOT MODIFY THIS CELL\nclass Config:\n    \"\"\"\n    A recursive configuration class that converts a dictionary into an object\n    with attributes accessible using dot notation.\n    \"\"\"\n\n    def __init__(self, dictionary):\n        for key, value in dictionary.items():\n            if isinstance(value, dict):\n                value = Config(value)\n            setattr(self, key, value)\n\ndef set_seed(seed=531):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\nset_seed()","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:16:47.336493Z","iopub.execute_input":"2025-04-02T09:16:47.337025Z","iopub.status.idle":"2025-04-02T09:16:47.345226Z","shell.execute_reply.started":"2025-04-02T09:16:47.336998Z","shell.execute_reply":"2025-04-02T09:16:47.344496Z"},"papermill":{"duration":5.859064,"end_time":"2025-03-06T04:14:05.672118","exception":false,"start_time":"2025-03-06T04:13:59.813054","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"config = {\n    # experiment configurations\n    \"exp_name\": \"ML2025_HW2\",\n    \"data_dir\":  Path(\"ML2025Spring-hw2-public\").resolve(),\n\n    # the description of the task\n    \"task_goal\": \"Given the survey results from the past two days in a specific state in the U.S.,\\\n                  predict the probability of testing positive on day 3. \\\n                  The evaluation metric is Mean Squared Error (MSE).\",\n\n    \"agent\": {\n        # the number of iterations\n        \"steps\": 1,\n        \"search\": {\n            # decide whether to debug or improve\n            \"debug_prob\": 0.5,\n            # the number of draft generated before improving/debugging\n            \"num_drafts\": 1,\n        },\n    },\n}\n\ncfg = Config(config)","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:16:54.347886Z","iopub.execute_input":"2025-04-02T09:16:54.348179Z","iopub.status.idle":"2025-04-02T09:16:54.352835Z","shell.execute_reply.started":"2025-04-02T09:16:54.348157Z","shell.execute_reply":"2025-04-02T09:16:54.351919Z"},"papermill":{"duration":0.026509,"end_time":"2025-03-06T04:14:05.718399","exception":false,"start_time":"2025-03-06T04:14:05.69189","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":"## Main","metadata":{"papermill":{"duration":0.019135,"end_time":"2025-03-06T04:14:05.756888","exception":false,"start_time":"2025-03-06T04:14:05.737753","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def main():\n\n    def exec_callback(*args, **kwargs):\n        res = interpreter.run(*args, **kwargs)\n        return res\n\n    journal = Journal()\n    agent = Agent(\n        cfg=cfg,\n        journal=journal,\n    )\n\n    interpreter = Interpreter()\n\n    global_step = len(journal)\n    while global_step < cfg.agent.steps:\n        # run agent\n        agent.step(exec_callback=exec_callback)\n        # save results for this iteration\n        save_run(cfg, journal)\n        # get currect step\n        global_step = len(journal)\n\n\n    # Kill created child process\n    interpreter.cleanup_session()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2025-04-02T09:17:01.896356Z","iopub.execute_input":"2025-04-02T09:17:01.896640Z","iopub.status.idle":"2025-04-02T09:17:44.967169Z","shell.execute_reply.started":"2025-04-02T09:17:01.896619Z","shell.execute_reply":"2025-04-02T09:17:44.966224Z"},"papermill":{"duration":68.699517,"end_time":"2025-03-06T04:15:14.475928","exception":false,"start_time":"2025-03-06T04:14:05.776411","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T09:18:05.660010Z","iopub.execute_input":"2025-04-02T09:18:05.660323Z","iopub.status.idle":"2025-04-02T09:18:05.800434Z","shell.execute_reply.started":"2025-04-02T09:18:05.660298Z","shell.execute_reply":"2025-04-02T09:18:05.799613Z"}},"outputs":[{"name":"stdout","text":"best_solution.py\t ML2025Spring-hw2-public.zip\ngood_solution_0.py\t qwen2.5-coder-7b-instruct-q8_0.gguf\nML2025Spring-hw2-public  submission.csv\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# Get your best result!\n# !python best_solution.py","metadata":{"execution":{"iopub.execute_input":"2025-03-06T04:15:14.51356Z","iopub.status.busy":"2025-03-06T04:15:14.513249Z","iopub.status.idle":"2025-03-06T04:15:14.51698Z","shell.execute_reply":"2025-03-06T04:15:14.51602Z"},"papermill":{"duration":0.023419,"end_time":"2025-03-06T04:15:14.518342","exception":false,"start_time":"2025-03-06T04:15:14.494923","status":"completed"},"tags":[]},"outputs":[],"execution_count":null}]}
